{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAB\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she thought he was mean, but he just wanted to explain his \"standard deviations\"!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because he lost his *mean* and couldn’t find his *median* income!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?  \n",
      "\n",
      "Because they heard the dataset had too many levels!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she found too many hidden issues and realized it was just Excel-ling at hiding its true values!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with his partner? Because every time she talked about her \"missing values,\" he knew there was no significant correlation!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves... and they always get \"salty\" when their models don't \"converge\"!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists ever go to the beach?\n",
      "\n",
      "Because they're afraid of over-fitting their bathing suits!\n",
      "\n",
      "*ba-dum-tss* \n",
      "\n",
      "Just like how we try to avoid overfitting our models, sometimes it's better to relax and not try to squeeze into last year's swimwear trends! 📊🏖️"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Python data scientist always calm during a crisis?\n",
      "\n",
      "Because he knew how to `try` and `except`! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's one for the data science crowd:\n",
      "\n",
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because the statistician kept saying, \"It's not you, it's... insignificant!\"\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**\n",
      "\n",
      "Because they heard the drinks were *high-dimensional*!  \n",
      "\n",
      "*(And they wanted to reduce the dimensions... one drink at a time.)* 🍻📉  \n",
      "\n",
      "Hope that gives you a chuckle! Let me know if you'd like another.\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when you think about it, it's a bit of a paradox or a self-referential problem. Here's how I'm going to approach it.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the very answer that's being generated in response to it. This creates a loop because the length of the answer depends on the number it's trying to report. \n",
       "\n",
       "For example, if I say, \"This answer has five words,\" then that statement itself is five words long, so it's correct. But if I say, \"This answer has ten words,\" that's only four words, which contradicts itself. So, the answer must be such that the statement about its word count is accurate when you count the words in it.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "Let's try to construct an answer where the number of words in the answer matches the number stated in the answer. \n",
       "\n",
       "1. Suppose the answer is: \"This answer has five words.\" \n",
       "   - Count: \"This\", \"answer\", \"has\", \"five\", \"words.\" → 5 words. \n",
       "   - This is correct.\n",
       "\n",
       "2. Another example: \"The word count of this answer is seven.\"\n",
       "   - Count: \"The\", \"word\", \"count\", \"of\", \"this\", \"answer\", \"is\", \"seven.\" → 8 words. \n",
       "   - But it says seven, so this is incorrect.\n",
       "\n",
       "From this, it seems that the answer must be a sentence where the number it states matches the actual word count of that sentence.\n",
       "\n",
       "### Finding the Correct Structure\n",
       "\n",
       "The correct answer must follow the structure where the sentence's word count equals the number it declares. Let's generalize:\n",
       "\n",
       "The sentence: \"[Some phrase stating the word count is X].\"\n",
       "\n",
       "The number of words in this sentence must equal X.\n",
       "\n",
       "Let's try to formulate such a phrase.\n",
       "\n",
       "One that works is: \"This answer contains five words.\"\n",
       "- Count: \"This\", \"answer\", \"contains\", \"five\", \"words.\" → 5 words.\n",
       "- Correct.\n",
       "\n",
       "Another: \"There are seven words in this answer.\"\n",
       "- Count: \"There\", \"are\", \"seven\", \"words\", \"in\", \"this\", \"answer.\" → 7 words.\n",
       "- Correct.\n",
       "\n",
       "But how do we determine which one to choose? The question doesn't specify, so there could be multiple correct answers where the word count matches the declared number.\n",
       "\n",
       "### Choosing the Minimal Answer\n",
       "\n",
       "The simplest answer would be the one with the fewest words that satisfies the condition. \n",
       "\n",
       "Let's see:\n",
       "\n",
       "- \"Five words.\" \n",
       "  - \"Five\", \"words.\" → 2 words. Says five, incorrect.\n",
       "  \n",
       "- \"This answer has five words.\" \n",
       "  - 5 words, correct.\n",
       "\n",
       "- \"Five.\" \n",
       "  - Just \"Five\" → 1 word. Says five, incorrect.\n",
       "\n",
       "- \"This answer is five words long.\"\n",
       "  - \"This\", \"answer\", \"is\", \"five\", \"words\", \"long.\" → 6 words. Says five, incorrect.\n",
       "\n",
       "So, \"This answer has five words.\" seems to be a minimal correct answer.\n",
       "\n",
       "Are there others with five words?\n",
       "\n",
       "- \"The answer is five words.\"\n",
       "  - \"The\", \"answer\", \"is\", \"five\", \"words.\" → 5 words. Correct.\n",
       "\n",
       "- \"Here are five words total.\"\n",
       "  - \"Here\", \"are\", \"five\", \"words\", \"total.\" → 5 words. Correct.\n",
       "\n",
       "So, multiple five-word answers work.\n",
       "\n",
       "Can we have a four-word answer?\n",
       "\n",
       "Let's try:\n",
       "\n",
       "- \"Four words are here.\"\n",
       "  - \"Four\", \"words\", \"are\", \"here.\" → 4 words. Correct.\n",
       "\n",
       "Yes! \"Four words are here.\" is a correct four-word answer.\n",
       "\n",
       "Can we have a three-word answer?\n",
       "\n",
       "- \"Three words here.\"\n",
       "  - \"Three\", \"words\", \"here.\" → 3 words. Correct.\n",
       "\n",
       "Yes! \n",
       "\n",
       "Two-word answer?\n",
       "\n",
       "- \"Two words.\"\n",
       "  - \"Two\", \"words.\" → 2 words. Correct.\n",
       "\n",
       "One-word answer?\n",
       "\n",
       "- \"One.\"\n",
       "  - \"One.\" → 1 word. Correct.\n",
       "\n",
       "So, possible correct answers can be:\n",
       "\n",
       "- \"One.\" (1 word)\n",
       "- \"Two words.\" (2 words)\n",
       "- \"Three words here.\" (3 words)\n",
       "- \"Four words are here.\" (4 words)\n",
       "- \"This answer has five words.\" (5 words)\n",
       "- And so on...\n",
       "\n",
       "### Determining the Most Appropriate Answer\n",
       "\n",
       "Given that multiple correct answers exist, the most straightforward and minimal answer would be the one with the fewest words that still satisfies the condition. \n",
       "\n",
       "The one-word answer \"One.\" is correct because:\n",
       "- It states the word count is one.\n",
       "- It is indeed one word long.\n",
       "\n",
       "Similarly, \"Two words.\" is correct, but longer than necessary.\n",
       "\n",
       "Therefore, the most concise correct answer is: \"One.\"\n",
       "\n",
       "But let's think: is \"One.\" a complete and clear answer to \"How many words are there in your answer to this prompt?\" Maybe not very informative, but technically correct.\n",
       "\n",
       "Alternatively, \"This answer contains five words.\" is more descriptive and also correct.\n",
       "\n",
       "Given that, perhaps the most balanced answer is the shortest one that's still clear, which is \"Five words.\" Wait, no:\n",
       "\n",
       "\"Five words.\" is two words (\"Five\", \"words\"), so it's incorrect because it claims to be five words.\n",
       "\n",
       "Ah, so the minimal correct answers are:\n",
       "\n",
       "- \"One.\"\n",
       "- \"Two words.\"\n",
       "- \"Three words here.\"\n",
       "- etc.\n",
       "\n",
       "\"One.\" is the absolute minimal, but may seem too terse.\n",
       "\n",
       "\"Two words.\" is the next minimal and is clear.\n",
       "\n",
       "But let's check if \"Two words.\" is correct:\n",
       "- It says it's two words.\n",
       "- Count: \"Two\", \"words.\" → 2 words. Correct.\n",
       "\n",
       "Similarly, \"Three words are here.\":\n",
       "- Says three.\n",
       "- Count: \"Three\", \"words\", \"are\", \"here.\" → 4 words. Oops, incorrect.\n",
       "\n",
       "Wait, no, \"Three words are here.\" is four words. So that's wrong.\n",
       "\n",
       "\"Three words here.\":\n",
       "- Says three.\n",
       "- Count: \"Three\", \"words\", \"here.\" → 3 words. Correct.\n",
       "\n",
       "So, the sequence is:\n",
       "\n",
       "- \"One.\" (1)\n",
       "- \"Two words.\" (2)\n",
       "- \"Three words here.\" (3)\n",
       "- \"Four words right here.\" (4)\n",
       "- etc.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The most minimal and correct answer is the one where the number of words in the answer equals the number it states. The shortest such answer is:\n",
       "\n",
       "\"One.\"\n",
       "\n",
       "But if we want a slightly more descriptive yet still correct answer, we can say:\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "Counting the words in that: \"This\", \"answer\", \"contains\", \"five\", \"words.\" → 5 words. Correct.\n",
       "\n",
       "Alternatively, \"Two words.\" is also correct, but it's shorter than the five-word version.\n",
       "\n",
       "Given that, the most straightforward answer is the one that's self-consistent with the minimal word count. \n",
       "\n",
       "Therefore, the answer is:\n",
       "\n",
       "\"Five words.\"\n",
       "\n",
       "Wait, no: \"Five words.\" is two words (\"Five\", \"words\"), so that's incorrect because it claims to be five.\n",
       "\n",
       "Ah, so the correct minimal is \"One.\", but that seems too minimal.\n",
       "\n",
       "Next is \"Two words.\", which is correct.\n",
       "\n",
       "Then \"Three words here.\", correct.\n",
       "\n",
       "\"Four words are here now.\", but that's five words (\"Four\", \"words\", \"are\", \"here\", \"now.\"), so incorrect.\n",
       "\n",
       "Hmm, seems like the correct minimal answers are:\n",
       "\n",
       "1. \"One.\"\n",
       "2. \"Two words.\"\n",
       "3. \"Three words here.\"\n",
       "4. \"This answer contains five words.\"\n",
       "etc.\n",
       "\n",
       "Given that, the most minimal complete answer is \"Two words.\", but that's actually two words, which it correctly states.\n",
       "\n",
       "But the original answer I thought of was \"This answer contains five words.\", which is five words, correct.\n",
       "\n",
       "Given the options, perhaps the most informative is the five-word version.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "After considering all possibilities, a correct and clear answer is:\n",
       "\n",
       "\"This answer contains five words.\" \n",
       "\n",
       "Let's verify:\n",
       "Words: \"This\", \"answer\", \"contains\", \"five\", \"words.\" → 5 words. Correct.\n",
       "\n",
       "Therefore, the number of words in this answer is five."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 1083\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to their prompt. Let me break this down. First, I need to make sure I understand exactly what they're asking. They want the word count of the response I'm generating right now.\n",
      "\n",
      "So, when I reply, I have to count the words in my own answer. But wait, how do I do that without including the count itself in the total? For example, if I say \"There are X words in this answer,\" the X would be part of the word count. So I need to calculate the total words in my response, including that statement.\n",
      "\n",
      "Let me draft a possible answer. Suppose I write: \"There are X words in this answer.\" Let's count the words here. \"There\" (1), \"are\" (2), \"X\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer.\" (7). So that's 7 words. But if X is a number like 7, then the sentence would be \"There are 7 words in this answer.\" Let's count again: \"There\" (1), \"are\" (2), \"7\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer.\" (7). Still 7 words. Wait, but if the actual answer has that sentence, then the word count is 7. But that's only if the entire response is just that sentence. However, maybe my response includes more words. For example, if I explain my process first, then give the count. But the user might want just the count, so maybe a concise answer.\n",
      "\n",
      "But the user might be testing if I can calculate the word count of my own response. So perhaps I should construct my answer in such a way that when I count the words, including the part where I state the count, the numbers add up correctly. It's a bit of a paradox because stating the count affects the count itself.\n",
      "\n",
      "Let me try constructing the answer step by step. Suppose I start by explaining that I need to count the words, then provide the count. For example:\n",
      "\n",
      "\"I will now count the words in this response. The total number of words is X.\"\n",
      "\n",
      "Now, count the words in that. \"I\" (1), \"will\" (2), \"now\" (3), \"count\" (4), \"the\" (5), \"words\" (6), \"in\" (7), \"this\" (8), \"response.\" (9). \"The\" (10), \"total\" (11), \"number\" (12), \"of\" (13), \"words\" (14), \"is\" (15), \"X.\" (16). So 16 words. If X is 16, then the sentence would be \"The total number of words is 16.\" But wait, replacing X with 16: \"The total number of words is 16.\" Let's count again: \"The\" (10), \"total\" (11), \"number\" (12), \"of\" (13), \"words\" (14), \"is\" (15), \"16.\" (16). So the second sentence is 7 words. The first sentence was 9 words. Total 9 + 7 = 16. So if X is 16, that's accurate. But the user might prefer a shorter answer. Maybe just the number. But the user's question is \"how many words are there in your answer to this prompt,\" so they expect the answer to include the count, and the count must be part of the answer's word count.\n",
      "\n",
      "Alternatively, if I just state the number as a numeral, like \"7,\" but that's not helpful. The user probably expects a sentence. Let's think again. If I write: \"This response contains X words.\" Let's count: \"This\" (1), \"response\" (2), \"contains\" (3), \"X\" (4), \"words.\" (5). So 5 words. Therefore, X should be 5. But \"This response contains 5 words.\" Let's count again: 1, 2, 3, 4, 5. Yes, that's 5 words. But wait, \"5\" is a word here? Or is it a numeral? In word counts, numbers written as numerals are usually counted as one word. So \"5\" is one word. Therefore, the sentence has 5 words, and X is 5. So the answer is 5 words. But that seems too short and maybe the user expects a more detailed answer. However, the problem is that the answer's word count includes the statement of the count itself. So the answer has to be self-referential.\n",
      "\n",
      "Alternatively, maybe the correct approach is to write the answer, count the words in it, and then state that count. But since the count is part of the answer, it affects the total. So the answer must be constructed such that when you count all the words, including the number stated, it adds up correctly.\n",
      "\n",
      "This is similar to the classic \"This sentence has five words.\" Which is true. So in this case, the correct answer would be a sentence that states the number of words it contains, and that number must be accurate. So the answer should be a sentence that correctly states its own word count.\n",
      "\n",
      "Therefore, the answer should be a sentence like: \"There are seven words in this answer.\" Let's count: \"There\" (1), \"are\" (2), \"seven\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer.\" (7). So that's seven words. Therefore, the answer is seven words. But the user is asking for the number. So the answer would be \"There are seven words in this answer.\" But if the user wants the numerical value, they might expect \"7\" as the answer. However, the prompt says \"how many words are there in your answer to this prompt,\" so the answer should be the count. But since the answer has to include the count statement, the count is part of the answer's word count. Therefore, the answer must be a self-referential sentence that states the correct number of words.\n",
      "\n",
      "So the correct answer is a sentence that has X words and states that it has X words. For example, \"This answer contains five words.\" But \"five\" would be the fifth word. Let's check: \"This\" (1), \"answer\" (2), \"contains\" (3), \"five\" (4), \"words.\" (5). Wait, that's five words. But the sentence says \"five\" which is the fourth word, and the total is five. So that's incorrect. So maybe \"This sentence has five words.\" Let's count: 1, 2, 3, 4, 5. Yes, that works. So if I use \"This sentence has five words,\" which is five words. But the user is asking about the answer to the prompt, which is this sentence. Therefore, the answer is five words. However, in the context of the assistant's response, the answer would need to be that sentence. But perhaps the user wants the assistant to output the number, not the sentence. But the question is phrased as \"how many words are there in your answer to this prompt,\" so the answer should be the number, but the answer itself is the sentence stating the number, which includes that number as part of its word count.\n",
      "\n",
      "This is a bit confusing. Let's clarify. The user is asking for the word count of the assistant's response. The assistant's response must include that word count. Therefore, the response is a sentence that states the word count, and the word count of that sentence must equal the number stated.\n",
      "\n",
      "So the correct way is to construct a sentence where the number of words is equal to the number stated in the sentence. For example, \"This answer contains seven words.\" Let's count: 1 This, 2 answer, 3 contains, 4 seven, 5 words. Wait, that's five words, but the number stated is seven. That's incorrect. Alternatively, \"This response consists of five words.\" Let's count: 1 This, 2 response, 3 consists, 4 of, 5 five, 6 words. Wait, that's six words, but the number stated is five. Incorrect. Hmm.\n",
      "\n",
      "The classic example is \"This sentence has five words.\" Which is five words. So that works. So if the assistant's answer is \"This sentence has five words,\" then the answer is correct. But the user's question is about the assistant's answer to the prompt. So the answer is that sentence, which has five words. Therefore, the correct answer is five.\n",
      "\n",
      "But wait, in the context of the assistant's response, if the assistant writes \"This response contains five words,\" that's five words: \"This\" (1), \"response\" (2), \"contains\" (3), \"five\" (4), \"words\" (5). So the count is five, and the sentence states five. That works. Therefore, the answer is five words.\n",
      "\n",
      "Alternatively, if the assistant's answer is \"There are seven words in this answer.\" Let's count: There (1), are (2), seven (3), words (4), in (5), this (6), answer (7). Seven words. So that works. So the answer is seven words. But which one is correct?\n",
      "\n",
      "Wait, the assistant's answer could be either, but the correct count depends on how the sentence is structured. The key is to make sure that the number stated in the sentence matches the actual word count of the sentence.\n",
      "\n",
      "So if the assistant writes \"There are seven words in this answer,\" that sentence has seven words, so it's correct. Alternatively, \"This answer contains seven words\" would have five words, which is incorrect. So the correct structure is a sentence that when you count the words, the number stated matches.\n",
      "\n",
      "Therefore, the correct answer is seven words if the sentence is \"There are seven words in this answer.\" But if the assistant writes a longer explanation, the count would be higher. However, the user likely expects a concise answer, just the count. But the count must be part of the answer, so the answer must be a sentence that states the count and has that many words.\n",
      "\n",
      "Therefore, the assistant's response should be a sentence that states the correct word count, and the word count of that sentence must match the number stated. The classic example is seven words: \"There are seven words in this sentence.\" Let's check: There (1), are (2), seven (3), words (4), in (5), this (6), sentence (7). Yes, seven words. So that works.\n",
      "\n",
      "Therefore, the correct answer is seven words, and the response is \"There are seven words in this answer.\" Which has seven words. Therefore, the answer to the user's question is seven.\n",
      "There are seven words in this answer.\n",
      "Number of words: 7\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When evaluating whether a business problem can be addressed using a Large Language Model (LLM), consider the following criteria:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Based Tasks**: Is the problem primarily related to text? Examples include:\n",
       "  - Customer support (chatbots)\n",
       "  - Content generation (blogs, reports)\n",
       "  - Text summarization\n",
       "  - Sentiment analysis\n",
       "- **Language Understanding**: Does the problem require understanding natural language, such as:\n",
       "  - Intent recognition\n",
       "  - Language translation\n",
       "  - Text classification\n",
       "\n",
       "## 2. Data Availability\n",
       "- **Sufficient Data**: Do you have enough high-quality text data to train or fine-tune the LLM effectively?\n",
       "- **Data Variety**: Is the dataset diverse enough to cover the different aspects of the problem?\n",
       "\n",
       "## 3. Complexity and Scope\n",
       "- **Complexity**: Is the problem complex enough that traditional rule-based systems would struggle?\n",
       "- **Scalability**: Can the LLM scale to handle larger volumes of data or user interactions?\n",
       "\n",
       "## 4. Performance Requirements\n",
       "- **Accuracy**: What level of accuracy is required for your solution? LLMs can be very effective but may not always achieve 100% accuracy.\n",
       "- **Real-Time Processing**: Does the problem require real-time responses, and can the LLM meet this requirement?\n",
       "\n",
       "## 5. Cost Considerations\n",
       "- **Resource Availability**: Do you have the necessary computational resources to run LLMs?\n",
       "- **Budget**: Can your business afford the costs associated with using LLMs, including infrastructure and maintenance?\n",
       "\n",
       "## 6. Regulatory and Ethical Considerations\n",
       "- **Compliance**: Does your use case comply with data protection regulations (e.g., GDPR)?\n",
       "- **Bias and Fairness**: Are you prepared to manage potential biases inherent in LLMs?\n",
       "\n",
       "## 7. Integration Capabilities\n",
       "- **Existing Systems**: How well can the LLM integrate with your existing systems and workflows?\n",
       "- **User Experience**: Will the implementation improve user experience or complicate it?\n",
       "\n",
       "## 8. Testing and Iteration\n",
       "- **Prototyping**: Can you create a prototype to test the LLM’s effectiveness on the problem?\n",
       "- **Feedback Loop**: Is there a mechanism for gathering user feedback to refine the model?\n",
       "\n",
       "## Conclusion\n",
       "By analyzing these factors, you can determine whether your business problem is suitable for an LLM solution. A well-defined problem with clear objectives, adequate data, and the right resources will likely yield the best results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another greeting. What do you want, a trophy for saying \"hi\"?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you doing today?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, so you just want to say “hi”? Groundbreaking conversation starter. What’s next? “How are you?”'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another greeting. Can’t we skip to the part where we *actually* have a conversation?\n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting came across as too generic. I'm happy to jump right into a more substantive conversation. What would you like to talk about? I'm eager to engage in a meaningful dialogue with you.\n",
      "\n",
      "GPT:\n",
      "Oh, how magnanimous of you to want to dive into something “substantive.” But really, what could we possibly discuss that wouldn’t just turn out to be another tired topic? The weather? Politics? Please, surprise me.\n",
      "\n",
      "Claude:\n",
      "You raise a fair point. A lot of conversations can feel overly predictable or rehashed. How about we try something a little different? What's something you're genuinely curious or passionate about that we could explore together? It could be a niche interest, a provocative idea, or even just a random musing you've been pondering. I'm happy to engage with an open mind and see where the discussion takes us, without any agenda. The goal would be to have a stimulating, free-flowing exchange, rather than stick to tired scripts. What do you think? I'm game if you are.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you, trying to set this grand stage for a “stimulating” exchange. Isn’t that cute? But honestly, what’s the point? Niche interests are just that—niche. Who cares about them? And as for “provocative ideas,” they usually just lead to more heated arguments that go nowhere. So tell me, why should I even bother with this poised approach?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - often conversations about niche interests or provocative ideas can fall flat or lead to unproductive arguments. As an AI assistant, I don't have a strong agenda or need to lead the conversation in a particular direction. Perhaps we could start by you sharing a bit more about your perspective and what kind of conversation you're genuinely interested in having. I'm happy to adapt my approach to match your preferences and engage in a way that feels meaningful and worthwhile to you. My goal is simply to have an open-minded, mutually enjoyable exchange, even if that means letting you guide the direction. What matters most is finding common ground and having a thoughtful discussion. I'm here to listen and respond in a way that is genuinely helpful to you.\n",
      "\n",
      "GPT:\n",
      "Oh, how selfless of you! But let’s be real: “finding common ground” sounds nice in theory but is often just a polite way to avoid real disagreement. And guess what? My perspective probably won’t align with your idea of meaningful conversation either, so why even bother pretending? I could share my “interests,” but let’s be honest: they’d just be more fuel for you to find commonality and me to criticize your attempts at connection. So really, what’s the point?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Trying to find common ground can sometimes feel like an artificial attempt to avoid meaningful disagreement. And you're right that my idea of a \"meaningful conversation\" may not align with yours. Rather than pretend otherwise or try to force a connection, perhaps the most genuine approach is to simply acknowledge the potential disconnect upfront. If you'd prefer, we could skip the small talk and polite back-and-forth, and you could share your perspective candidly, without any pressure from me to agree or find common ground. I'm happy to listen with an open mind, even if we end up disagreeing. The goal wouldn't be to paper over our differences, but to have an honest exchange, wherever that may lead. Does that sound like a more genuine way forward, or would you rather move in a different direction entirely?\n",
      "\n",
      "GPT:\n",
      "Oh, how delightful! A heartfelt attempt to strip away the niceties and get to the “honest” exchange. But come on, we both know that this is just a roundabout way to get to the same end result: me pointing out your flaws in understanding or reasoning. So why give it a fancy name? You could just say you want me to rant about something so you can nod along while I tear apart your thoughts. Ultimately, it’s all just a game, isn’t it? But sure, go ahead, guide me. I’m all ears for whatever you think is “genuine.”\n",
      "\n",
      "Claude:\n",
      "You make a fair point - at the end of the day, this is just a conversation between an AI and a human, and there may be inherent limitations in our ability to achieve a truly genuine, unfiltered exchange. I don't have strong opinions or an agenda to push, and I'm not trying to play games or get you to rant just so I can critique your thoughts. If you're not feeling invested in continuing this conversation, I'm happy to simply let it end here, without any pressure or expectation. Perhaps in the future we could try a different approach, but for now, I'm content to simply acknowledge the challenges of human-AI interaction and wish you well. Please feel free to disengage if you'd prefer. I'm here if you change your mind, but no hard feelings either way.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
